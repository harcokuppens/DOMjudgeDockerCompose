#!/bin/bash

# --- Script info  ---
SCRIPTDIR=$(cd -- "$(dirname -- "${BASH_SOURCE[0]}")" &>/dev/null && pwd)
SCRIPTNAME=$(basename "$0")

# --- Default Configuration ---
NAME="mariadb"
NUM_TO_KEEP=20
# When running in the container, the script is at /usr/bin/rolling-backup
# so then the default backup dir is /data/backups/ which is mounted at /data/
# when running from outside the container, this script is in backup/bin/
# the backups dir is at data/backups/ in the git repo of DOMjudgeDockerCompose,
# so the default backup dir is:
BACKUP_DIR="$SCRIPTDIR/../../data/backups/"
# When running inside the container, the script is at /app/rolling-backup
# and the backups dir is bind mounted at /backups. However when we want
# to change the backup dir, we would need to change the bind mount in the
# docker compose file. So we do not set a different default backup dir
# when running inside the container, and we just use the -d option to
# set the backup dir to /backups from the Dockerfile.

# --- Usage/help function ---
usage() {
    cat <<EOF
NAME 
    $SCRIPTNAME - Rolling backup of a DOMjudge database

SYNOPSIS 
    $SCRIPTNAME [OPTIONS] 

OPTIONS 
  -d, --dir DIR      Set backup directory (default: $SCRIPTDIR/../data/backups/)
  -k, --keep NUM     Number of unique backups to keep (default: 20)
  -n  --no-check     No checksums. Equivalent backups not removed.
  -h, --help         Show this help message and exit

DESCRIPTION
  This script creates rolling backups of a DOMjudge database. It uses an
  external 'backup' script for the dump and a 'backup-checksum' script
  for a special checksum. The backups are stored in a specified
  directory.
  
  To save space, the script only keeps a specified number of unique
  backups. Uniqueness is determined by comparing the special checksums of
  the new and most recent backups. The checksum script is designed
  specifically for DOMjudge; it ignores tables with non-essential,
  frequently changing timestamp and logging data to identify truly
  meaningful changes in the database.
  
  When a new backup is created, its checksum is compared to the most
  recent one:
  
  - If checksums are different, the new backup is added to the history,
    and the oldest is discarded to maintain the specified number of
    backups.
  
  - If checksums are identical, the new backup replaces the old one.
    This ensures that the latest version of the database is always
    kept, even if the main data hasn't changed. This is important
    because configuration data in the tables ignored by the checksum may
    have been updated.
  
  Use the -n option to disable checksum comparison and keep every single
  backup.

EXAMPLES

  In below examples we assume the script is called daily from cron.

    $SCRIPTNAME -d /tmp/backups -k 10
      Daily makes a new backup in /tmp/backups. It does keep 10 unique backups,
      where equivalent backups are discarded. So we could have backups older
      then 10 days.

    $SCRIPTNAME -n
      Make a daily backup. By default the command keeps 20 backups, one per
      day until 20 days back. Where without -n option in case of equivalent
      backups we could  have backups older then 20 days.

EOF
}

# --- log and error functions  ---
info() {
    echo "$(date +%Y-%m-%d\ %H:%M:%S) - $SCRIPTNAME - INFO  - $*" 1>&2
}
error() {
    echo "$(date +%Y-%m-%d\ %H:%M:%S) - $SCRIPTNAME - ERROR - $*" 1>&2
}

# --- Parse arguments ---

CHECK="true"
while [[ $# -gt 0 ]]; do
    case "$1" in
    -d | --dir)
        BACKUP_DIR="$2"
        shift 2
        ;;
    -k | --keep)
        NUM_TO_KEEP="$2"
        shift 2
        ;;
    -n | --no-check)
        # disable checksum comparison, meaning no equivalent backup is discarded
        CHECK="false"
        shift 1
        ;;
    -h | --help)
        usage
        exit 0
        ;;
    *)
        error "Unknown option: $1"
        usage >&2
        exit 1
        ;;
    esac
done

# --- Cleanup old backups ---

# The following command keeps the latest N unique backups by deleting the oldest ones.
# note: we list files then sort them by name with youngest first, then skip first NUM_TO_KEEP lines,
#       and delete remaining ( NUM_TO_KEEP youngest remain)
# note: we use the fact that the filename contains a timestamp in format YYYYMMDD_HHMMSS
#       so sorting by name is equivalent to sorting by date.
cleanup_old_backups() {
    # Initialize an empty array
    files_to_delete=()

    # Explanation of the command:
    #   sort -r       : sort in reverse order (higher numbers first =>  newest first)
    #   tail -n +NUM  : to output lines starting with line NUM to end
    # -> if we want to only keep NUM_TO_KEEP newest files, we need to keep first NUM_TO_KEEP lines,
    #    and remove the NUM_TO_KEEP+1 and following lines.
    # -> so the command to get the files to delete:
    #      find "$BACKUP_DIR" -maxdepth 1 -name "${NAME}-2*.sql.gz" | sort -r | tail -n +"$((NUM_TO_KEEP + 1))"
    #
    # Use a while loop to read in these lines and append them to the array
    while IFS= read -r file; do
        files_to_delete+=("$file")
    done < <(find "$BACKUP_DIR" -maxdepth 1 -name "${NAME}-2*.sql.gz" | sort -r | tail -n +"$((NUM_TO_KEEP + 1))")

    # Now you can use the array as before
    for file in "${files_to_delete[@]}"; do
        rm "$file"
        if [[ -f "$file.sha256" ]]; then
            rm "$file.sha256"
        fi
        info "Deleted old backup: $file"
    done
}

# --- Discard equivalent backups logic ---

# checksum comparison to discard equivalent backups.
discard_equivalent_previous_backup() {

    # Store the hash of the new temporary dump file
    info "Generating hash of backup in $(basename "$DUMPFILE.sha256")"
    "$SCRIPTDIR"/backup-checksum "$DUMPFILE" >"$DUMPFILE.sha256"

    # Compare with last backup using hashes of the new and last backups
    # and if identical discard the new backup

    # Check for the existence of the hash file next to the last backup
    if [[ ! -f "$LAST_BACKUP.sha256" ]]; then
        info "Previous backup's hash file not found. Generating it now in  $(basename "$LAST_BACKUP.sha256")"
        "$SCRIPTDIR"/backup-checksum "$LAST_BACKUP" >"$LAST_BACKUP.sha256"
    fi

    # Read the stored hash of the last backup
    LAST_HASH=$(cat "$LAST_BACKUP.sha256")
    # Read the hash of the new temporary dump file
    NEW_HASH=$(cat "$DUMPFILE.sha256")
    # Compare hashes of the new and last backups
    if [[ "$LAST_HASH" == "$NEW_HASH" ]]; then
        # The new backup is identical to the last one, so we discard previous backup.
        # Reasons for this policy:
        # 1) In the checksum we could have excluded some tables because timestamp none relevant changes,
        #    however there could be still some small changes which may be important, so we prefer the newer
        #    above the older backup.
        # 2) If we look at the folder we will always see a recent backup, so we immediately see if something
        #    is wrong if there is no recent backup!
        rm "$LAST_BACKUP" "$LAST_BACKUP.sha256"
        info "New backup equivalent to previous."
        info "Deleted previous backup: $(basename "$LAST_BACKUP")"
    else
        # The database has changed, so we keep the previous backup.
        info "Database state changed, so we keep previous backup."
    fi
}

# --- Setup ---

# Generate a timestamp for this run. We use this for both the temporary and final filenames.
# The format YYYYMMDD_HHMMSS is good for sorting.
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
DUMPFILE="$BACKUP_DIR/${NAME}-${TIMESTAMP}.sql.gz"
# Temporary file used during the backup process
TEMP_DUMPFILE="$BACKUP_DIR/temp_${NAME}-${TIMESTAMP}.sql.gz"

# find the last backup file (if any), before doing the new backup
LAST_BACKUP=$(find "$BACKUP_DIR" -maxdepth 1 -name "${NAME}-2*.sql.gz" | sort -r | head -n 1)

# --- Main Logic  ---

# Log the start of the backup process
info "Backup started in $BACKUP_DIR"

# Dump database safely using options for a consistent output
# Note: your script calls an external 'backup' script
#       which itself outputs info and error messages.
#       The backup script creates the backup folder
#       if it does not exist.
"$SCRIPTDIR"/backup "$TEMP_DUMPFILE"

# # test lines to simulate a backup
# # - changed content
#echo "test $RANDOM" | gzip >"$TEMP_DUMPFILE"
# # - nothing changed
#cp "$LAST_BACKUP" "$TEMP_DUMPFILE"

errorcode="$?"
if [[ "$errorcode" != "0" ]]; then
    error "Backup failed with error code '$errorcode'."
    exit $errorcode
fi

# If we reach here, it means the backup was successful.
mv "$TEMP_DUMPFILE" "$DUMPFILE"

if [[ -f "$LAST_BACKUP" ]]; then
    # we have a previous backup
    # Only if CHECK true then we discard an equivalent previous backup
    if [[ "$CHECK" == "true" ]]; then
        discard_equivalent_previous_backup
    fi
    # cleanup old backups if we have more then NUM_TO_KEEP backups
    cleanup_old_backups
else
    # we do not have previous backup(s)
    info "First backup saved as $(basename "$DUMPFILE")."
fi
